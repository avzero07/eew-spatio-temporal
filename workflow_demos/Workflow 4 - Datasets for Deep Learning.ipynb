{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb69549",
   "metadata": {},
   "source": [
    "# Workflow 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9056062e",
   "metadata": {},
   "source": [
    "### Import est_lib Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c94c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from est_lib.util.obspy_util import *\n",
    "from est_lib.util.obspy_plot import ray_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b35fdf2",
   "metadata": {},
   "source": [
    "### Import Some other est_lib Functions Used in this Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6279f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from est_lib.nn.cp_line_simple import cp_line_simple\n",
    "from est_lib.dataset.seismic_dataset_new import EQDataset, get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2593fd",
   "metadata": {},
   "source": [
    "### Import Some other Functions that are also used in this Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e3cf6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a2338",
   "metadata": {},
   "source": [
    "### Specify Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0616460",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_list = ['HOLB','PACB','WOSB','HOPB']\n",
    "channel_list = ['HHE','HHN','HHZ']\n",
    "\n",
    "# Note: channel_list specifies the channels of interest\n",
    "#    In this example, we retrieve velocity data for \n",
    "#    all three directions (E, N and Z). To get acceleration\n",
    "#    data replace with ['HNE','HNN','HNZ']\n",
    "\n",
    "'''\n",
    "Specify a Time of interest. Note that that the timestamp\n",
    "is in UTC.\n",
    "'''\n",
    "event_time = \"2019-12-25T03:36:01.578000Z\"\n",
    "event_lat = 50.6081\n",
    "event_lon = -129.9656\n",
    "event = ('6.3',event_lat,event_lon,event_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1b7b4f",
   "metadata": {},
   "source": [
    "### Retrieve Station Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a929597",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use the inventory_retriever function to retrieve\n",
    "metadata about stations in station_list\n",
    "'''\n",
    "station_metadata = inventory_retriever(sta_list=station_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "264b1d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inventory created at 2021-08-06T03:09:25.000000Z\n",
      "\tCreated by: IRIS WEB SERVICE: fdsnws-station | version: 1.1.47\n",
      "\t\t    http://service.iris.edu/fdsnws/station/1/query?network=CN&station=H...\n",
      "\tSending institution: IRIS-DMC (IRIS-DMC)\n",
      "\tContains:\n",
      "\t\tNetworks (1):\n",
      "\t\t\tCN\n",
      "\t\tStations (4):\n",
      "\t\t\tCN.HOLB (Holberg, BC, CA)\n",
      "\t\t\tCN.HOPB (Hope, BC, CA)\n",
      "\t\t\tCN.PACB (Port Alice, BC, CA)\n",
      "\t\t\tCN.WOSB (Woss, BC, CA)\n",
      "\t\tChannels (65):\n",
      "\t\t\tCN.HOLB..EHZ, CN.HOLB..HHZ (4x), CN.HOLB..HHN (4x), \n",
      "\t\t\tCN.HOLB..HHE (4x), CN.HOLB..HNZ (3x), CN.HOLB..HNN (3x), \n",
      "\t\t\tCN.HOLB..HNE (3x), CN.HOPB..BHZ (2x), CN.HOPB..BHN (2x), \n",
      "\t\t\tCN.HOPB..BHE (2x), CN.HOPB..HHZ (2x), CN.HOPB..HHN (2x), \n",
      "\t\t\tCN.HOPB..HHE (2x), CN.HOPB..HNZ (2x), CN.HOPB..HNN (2x), \n",
      "\t\t\tCN.HOPB..HNE (2x), CN.PACB..HHZ (2x), CN.PACB..HHN (2x), \n",
      "\t\t\tCN.PACB..HHE (2x), CN.PACB..HNZ (2x), CN.PACB..HNN (2x), \n",
      "\t\t\tCN.PACB..HNE (2x), CN.WOSB..EHZ, CN.WOSB..HHZ, CN.WOSB..HHN, \n",
      "\t\t\tCN.WOSB..HHE, CN.WOSB..HNZ (3x), CN.WOSB..HNN (3x), \n",
      "\t\t\tCN.WOSB..HNE (3x)\n"
     ]
    }
   ],
   "source": [
    "print(station_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51218207",
   "metadata": {},
   "source": [
    "### Retrieve Seismic Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "446efb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use the stream_retriever function to retrieve\n",
    "seismic streams from an IRIS' FDSN compliant\n",
    "datacenter.\n",
    "\n",
    "We instruct the function to fetch data starting\n",
    "from 500 seconds before the specified time upto\n",
    "150 seconds after.\n",
    "\n",
    "That is, if the time of interest is at 0, we\n",
    "fetch data from -500 to 1500.\n",
    "'''\n",
    "\n",
    "seconds_before = 500\n",
    "seconds_after = 1500\n",
    "\n",
    "'''\n",
    "Datasets for supervised deep learning have a training\n",
    "set and a test set. Let us split the data we retrieve\n",
    "such that data related to all but the last station (HOPB)\n",
    "in station_list forms the training set and the data from\n",
    "HOPB forms the test set.\n",
    "'''\n",
    "\n",
    "train_data = stream_retriever(event_time=event_time,\n",
    "                        time_format='string',\n",
    "                        seconds_before = 500,\n",
    "                        seconds_after = 1500,\n",
    "                        sta_list = station_list[0:-1],\n",
    "                        channel_list = channel_list)\n",
    "\n",
    "test_data = stream_retriever(event_time=event_time,\n",
    "                        time_format='string',\n",
    "                        seconds_before = 500,\n",
    "                        seconds_after = 1500,\n",
    "                        sta_list = station_list[-1:],\n",
    "                        channel_list = channel_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed86e34",
   "metadata": {},
   "source": [
    "### Write Streams to Local Filesystem\n",
    "\n",
    "#### Specify the Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c973790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.abspath(os.path.join('./Data'))\n",
    "if(not os.path.isdir(filepath)):\n",
    "    os.mkdir(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa5cf34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksha\\Desktop\\eew-spatio-temporal\\workflow_demos\\Data\n"
     ]
    }
   ],
   "source": [
    "print(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03db2e90",
   "metadata": {},
   "source": [
    "#### Write Train File to File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5fe060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_f_path = os.path.join(filepath,'train_x.npy')\n",
    "train_x_file = stream_data_writer(train_data,\n",
    "                              station_metadata,\n",
    "                              train_x_f_path,\n",
    "                              station_list[0:-1],\n",
    "                              channel_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b4f76f",
   "metadata": {},
   "source": [
    "#### Write Test File to File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20be4687",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_f_path = os.path.join(filepath,'train_y.npy')\n",
    "train_y_file = stream_data_writer(test_data,\n",
    "                              station_metadata,\n",
    "                              train_y_f_path,\n",
    "                              station_list[-1:],\n",
    "                              channel_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9181497b",
   "metadata": {},
   "source": [
    "### Write Inventory Data to Local Filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "228f1161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\aksha\\\\Desktop\\\\eew-spatio-temporal\\\\workflow_demos\\\\Data\\\\demo_stations_metadata.xml'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Use inventory_writer to save station_metadata to a\n",
    "file in the local filesystem in the XML format.\n",
    "Specify a path to the new file as the second argument.\n",
    "'''\n",
    "metadata_file = os.path.join(filepath,'demo_stations_metadata.xml')\n",
    "inventory_writer(station_metadata,metadata_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7e1f70",
   "metadata": {},
   "source": [
    "### Initialize a PyTorch Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd7b9a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65065e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EQDataset(metadata_file,\n",
    "                    train_x_file,train_y_file,\n",
    "                    sta_list=station_list,\n",
    "                    chan_list=channel_list,\n",
    "                    ip_dim=len(channel_list),\n",
    "                    num_nodes=len(station_list[0:-1]),\n",
    "                    seq_length=500,horizon=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecb2f31",
   "metadata": {},
   "source": [
    "### Initialize a Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e4a6a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1fa774",
   "metadata": {},
   "source": [
    "### Initialize a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34228b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cp_line_simple(num_in_nodes=len(station_list[0:-1]),\n",
    "                     feat_size=len(channel_list)).to(get_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc94f6a5",
   "metadata": {},
   "source": [
    "### Pass a Datapoint from the PyTorch Dataset into the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77faff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_point = next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a426f96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.4189]], device='cuda:0', grad_fn=<AddmmBackward>),\n",
       " tensor([[0.2631]], device='cuda:0', grad_fn=<AddmmBackward>),\n",
       " tensor([[-0.2421]], device='cuda:0', grad_fn=<AddmmBackward>)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(data_point[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3be6ddf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 508., -320., -300.]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(data_point[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a313522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
